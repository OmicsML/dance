{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Adding and Using a Custom Algorithm in the Auto-Search Framework\n",
    "\n",
    "In this notebook, we'll walk through how to integrate a new algorithm (specifically an SVM classifier) into the auto-search framework outlined in your documentation. We will:\n",
    "\n",
    "1. Inherit from the **BaseClassificationMethod** (or another suitable base) to define our custom method. Implement the required interfaces (`fit`, `predict`, and optionally `preprocessing_pipeline`).  \n",
    "2. Show how to run the hyperparameter search using the integrated method.  \n",
    "3. Provide an example `main.py`-like script that demonstrates how the auto-search process is orchestrated.\n",
    "\n",
    "## 1. Folder Structure & Requirements\n",
    "\n",
    "Before diving in, ensure you have the following directory structure (at least conceptually; your actual project structure can be more extensive):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "examples/tuning/\n",
    "└── classification_svm/\n",
    "    ├── main.py\n",
    "    ├── tutorial.ipynb  \n",
    "    └── dataset_name/\n",
    "        ├── pipeline_params_tuning_config.yaml\n",
    "        └── config_yamls/\n",
    "            ├── 0_test_acc_params_tuning_config.yaml\n",
    "            ├── 1_test_acc_params_tuning_config.yaml\n",
    "            └── 2_test_acc_params_tuning_config.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `cta_svm` is the directory we created for our new algorithm. The same pattern can apply for other methods, such as `clustering_kmeans`, `regression_linreg`, etc.\n",
    "\n",
    "We'll focus on the **SVM** example below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining Our SVM Classifier\n",
    "\n",
    "Suppose we want to define a custom SVM method for classification.\n",
    "We'll inherit from BaseClassificationMethod and implement the required methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dance.modules.base import BaseClassificationMethod\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "from dance.transforms.cell_feature import WeightedFeaturePCA\n",
    "from dance.transforms.misc import Compose, SetConfig\n",
    "from dance.typing import LogLevel\n",
    "\n",
    "class SVM(BaseClassificationMethod):\n",
    "    \"\"\"The SVM cell-type classification model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : argparse.Namespace\n",
    "        A Namespace contains arguments of SVM. See parser help document for more info.\n",
    "    prj_path: str\n",
    "        project path\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, prj_path=\"./\", random_state: Optional[int] = None):\n",
    "        self.args = args\n",
    "        self.random_state = random_state\n",
    "        self._mdl = SVC(random_state=random_state, probability=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocessing_pipeline(n_components: int = 400, log_level: LogLevel = \"INFO\"):\n",
    "        return Compose(\n",
    "            WeightedFeaturePCA(n_components=n_components, split_name=\"train\"),\n",
    "            SetConfig({\n",
    "                \"feature_channel\": \"WeightedFeaturePCA\",\n",
    "                \"label_channel\": \"cell_type\"\n",
    "            }),\n",
    "            log_level=log_level,\n",
    "        )\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Train the classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Training cell features.\n",
    "        y\n",
    "            Training labels.\n",
    "\n",
    "        \"\"\"\n",
    "        self._mdl.fit(x, y)\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"Predict cell labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            Samples to be predicted (samplex x features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y\n",
    "            Predicted labels of the input samples.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._mdl.predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example `main.py` File\n",
    "\n",
    "Below is an example of how your `main.py` might look if you're adding SVM as one of the classification methods. This file orchestrates the entire pipeline:\n",
    "\n",
    "1. **Register** preprocessing functions through annotations (optional)\n",
    "2. **Parsing Arguments** and configuring hyperparameters.  \n",
    "3. **Defining** an evaluation function that:  \n",
    "   - Loads and preprocesses the data.  \n",
    "   - Initializes your model (the new SVM class).  \n",
    "   - Trains and scores the model.  \n",
    "   - Logs results to Weights & Biases (wandb).  \n",
    "4. **Running** the hyperparameter sweep agent (e.g., via `wandb_sweep_agent`).  \n",
    "5. **Saving** results and optionally generating a second-stage tuning config file.\n",
    "\n",
    "> **Note**: For demonstration, only relevant code is shown. Adjust as needed for your exact pipeline or data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Step 1: preprocessing functions can be registered using register_preprocessor. \n",
    "In this example, the GaussRandProjFeature preprocessing function is registered within the feature.cell pipeline. \n",
    "This registered function can later be specified in the configuration file.\n",
    "\"\"\"\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from dance.registry import register_preprocessor\n",
    "from dance.transforms.base import BaseTransform\n",
    "\n",
    "\n",
    "@register_preprocessor(\"feature\", \"cell\",overwrite=True)  # NOTE: register any custom preprocessing function to be used for tuning\n",
    "class GaussRandProjFeature(BaseTransform):\n",
    "    \"\"\"Custom preprocessing to extract cell feature via Gaussian random projection.\"\"\"\n",
    "\n",
    "    _DISPLAY_ATTRS = (\"n_components\", \"eps\")\n",
    "\n",
    "    def __init__(self, n_components: int = 400, eps: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_components = n_components\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, data):\n",
    "        feat = data.get_feature(return_type=\"numpy\")\n",
    "        grp = GaussianRandomProjection(n_components=self.n_components, eps=self.eps)\n",
    "\n",
    "        self.logger.info(f\"Start generateing cell feature via Gaussian random projection (d={self.n_components}).\")\n",
    "        data.data.obsm[self.out] = grp.fit_transform(feat)\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example main.py\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import get_args\n",
    "\n",
    "from dance.registry import register_preprocessor\n",
    "from dance.transforms.base import BaseTransform\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "from dance import logger\n",
    "from dance.datasets.singlemodality import CellTypeAnnotationDataset  # your dataset\n",
    "from dance.pipeline import PipelinePlaner, get_step3_yaml, run_step3, save_summary_data\n",
    "from dance.utils import set_seed\n",
    "from dance.typing import LogLevel\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "root_path=str(Path(__file__).resolve().parent) if '__file__' in globals() else Path(\"tutorial.ipynb\").resolve().parent\n",
    "\n",
    "# Import your custom SVM class\n",
    "# In reality, you'd do: from your_svm_file import SVM\n",
    "# from your_svm_file import SVM\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    #Step 2: Parsing Arguments and configuring hyperparameters\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--cache\", action=\"store_true\", help=\"Cache processed data.\")\n",
    "    parser.add_argument(\"--dense_dim\", type=int, default=400, help=\"dim of PCA\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU id, set to -1 for CPU\")\n",
    "    parser.add_argument(\"--log_level\", type=str, default=\"INFO\", choices=get_args(LogLevel))\n",
    "    parser.add_argument(\"--species\", default=\"human\")\n",
    "    parser.add_argument(\"--test_dataset\", nargs=\"+\", default=[138], type=int, help=\"list of dataset id\")\n",
    "    parser.add_argument(\"--tissue\", default=\"Brain\")  # TODO: Add option for different tissue name for train/test\n",
    "    parser.add_argument(\"--train_dataset\", nargs=\"+\", default=[328], type=int, help=\"list of dataset id\")\n",
    "    parser.add_argument(\"--valid_dataset\", nargs=\"+\", default=None, type=int, help=\"list of dataset id\")\n",
    "    parser.add_argument(\"--tune_mode\", default=\"pipeline_params\", choices=[\"pipeline\", \"params\", \"pipeline_params\"])\n",
    "    parser.add_argument(\"--seed\", type=int, default=10)\n",
    "    parser.add_argument(\"--count\", type=int, default=2)\n",
    "    parser.add_argument(\"--sweep_id\", type=str, default=None)\n",
    "    parser.add_argument(\"--summary_file_path\", default=\"results/pipeline/best_test_acc.csv\", type=str)\n",
    "    parser.add_argument(\"--root_path\", default=root_path, type=str)\n",
    "    if args is None:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(args)\n",
    "\n",
    "    # Construct the path to the tuning config file\n",
    "    file_root_path = Path(\n",
    "        args.root_path, \"_\".join([\n",
    "            \"-\".join([str(num) for num in dataset])\n",
    "            for dataset in [args.train_dataset, args.valid_dataset, args.test_dataset] if dataset is not None\n",
    "        ])).resolve()\n",
    "    logger.info(f\"\\n files is saved in {file_root_path}\")\n",
    "\n",
    "    # Instantiate pipeline planer from config file\n",
    "    pipeline_planer = PipelinePlaner.from_config_file(f\"{file_root_path}/{args.tune_mode}_tuning_config.yaml\")\n",
    "    os.environ[\"WANDB_AGENT_MAX_INITIAL_FAILURES\"] = \"2000\"\n",
    "\n",
    "    #Step 3: define evaluation function\n",
    "    def evaluate_pipeline(tune_mode=args.tune_mode, pipeline_planer=pipeline_planer):\n",
    "        \"\"\"\n",
    "        The evaluation function used by wandb_sweep_agent.\n",
    "        It:\n",
    "        1. Loads data.\n",
    "        2. Applies the pipeline.\n",
    "        3. Trains and scores the model.\n",
    "        4: Evaluate model\n",
    "        5. Logs metric(s) to wandb.\n",
    "        \"\"\"\n",
    "        wandb.init(settings=wandb.Settings(start_method='thread'))\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        # Load dataset\n",
    "        data = CellTypeAnnotationDataset(train_dataset=args.train_dataset, test_dataset=args.test_dataset,\n",
    "                                         valid_dataset=args.valid_dataset, species=args.species, tissue=args.tissue,\n",
    "                                         data_dir=\"../temp_data\").load_data()\n",
    "\n",
    "        # Preprocessing pipeline\n",
    "        kwargs = {tune_mode: dict(wandb.config)}\n",
    "        preprocessing_pipeline = pipeline_planer.generate(**kwargs)\n",
    "        preprocessing_pipeline(data)\n",
    "\n",
    "        # Retrieve training / testing data\n",
    "        x_train, y_train = data.get_train_data()\n",
    "        y_train_converted = y_train.argmax(1)\n",
    "        x_valid, y_valid = data.get_val_data()\n",
    "        x_test, y_test = data.get_test_data()\n",
    "\n",
    "        #Initialize our custom SVM model and train\n",
    "        # from your_svm_file import SVM  # Place your SVM import here\n",
    "        model = SVM(args, random_state=args.seed)\n",
    "        model.fit(x_train, y_train_converted)\n",
    "\n",
    "        #Evaluate model\n",
    "        train_score = model.score(x_train, y_train)\n",
    "        score = model.score(x_valid, y_valid)\n",
    "        test_score = model.score(x_test, y_test)\n",
    "\n",
    "        #Log results to wandb\n",
    "        wandb.log({\"train_acc\": train_score, \"acc\": score, \"test_acc\": test_score})\n",
    "        wandb.finish()\n",
    "\n",
    "    # Step 4: Run the sweep\n",
    "    entity, project, sweep_id = pipeline_planer.wandb_sweep_agent(\n",
    "        evaluate_pipeline, sweep_id=args.sweep_id, count=args.count) \n",
    "\n",
    "    #Step 5: Save summary data (top results, etc.)\n",
    "    save_summary_data(entity, project, sweep_id, summary_file_path=args.summary_file_path, root_path=file_root_path)\n",
    "\n",
    "    # Optionally, handle pipeline + parameter search steps\n",
    "    if args.tune_mode == \"pipeline\" or args.tune_mode == \"pipeline_params\":\n",
    "        get_step3_yaml(result_load_path=f\"{args.summary_file_path}\", step2_pipeline_planer=pipeline_planer,\n",
    "                       conf_load_path=f\"{Path(args.root_path).resolve().parent}/step3_default_params.yaml\",\n",
    "                       root_path=file_root_path)\n",
    "        if args.tune_mode == \"pipeline_params\":\n",
    "            run_step3(file_root_path, evaluate_pipeline, tune_mode=\"params\", step2_pipeline_planer=pipeline_planer)\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    os.environ[\"http_proxy\"] = \"http://121.250.209.147:7890\"\n",
    "    os.environ[\"https_proxy\"] = \"http://121.250.209.147:7890\"\n",
    "    main([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Files\n",
    "\n",
    "The **configuration files** (e.g., `pipeline_params_tuning_config.yaml`, `pipeline_tuning_config.yaml`, `params_tuning_config.yaml`) guide the auto-search. Each file contains instructions for how to vary your preprocessing pipeline or model hyperparameters (or both). For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pipeline_params_tuning_config.yaml\n",
    "```yaml\n",
    "type: preprocessor\n",
    "tune_mode: pipeline_params\n",
    "pipeline_tuning_top_k: 2\n",
    "parameter_tuning_freq_n: 2\n",
    "pipeline:\n",
    "  - type: filter.gene\n",
    "    include:\n",
    "      - FilterGenesPercentile\n",
    "      - FilterGenesScanpyOrder\n",
    "      - FilterGenesPlaceHolder\n",
    "    default_params:\n",
    "      FilterGenesScanpyOrder:\n",
    "          order: [\"min_counts\", \"min_cells\", \"max_counts\", \"max_cells\"]\n",
    "          min_counts: 1\n",
    "          max_counts: 134732\n",
    "          min_cells: 1\n",
    "          max_cells: 401\n",
    "  - type: normalize\n",
    "    include:\n",
    "      - ScaleFeature\n",
    "      - ScTransform\n",
    "      - Log1P\n",
    "      - NormalizeTotal\n",
    "      - NormalizePlaceHolder\n",
    "    default_params:\n",
    "      ScTransform:\n",
    "        processes_num: 8\n",
    "  - type: filter.gene\n",
    "    include:\n",
    "      # - HighlyVariableGenesLogarithmizedByMeanAndDisp\n",
    "      - HighlyVariableGenesRawCount\n",
    "      - HighlyVariableGenesLogarithmizedByTopGenes\n",
    "      - FilterGenesTopK\n",
    "      - FilterGenesRegression\n",
    "      # - FilterGenesNumberPlaceHolder\n",
    "    default_params:\n",
    "      FilterGenesTopK:\n",
    "        num_genes: 100\n",
    "      FilterGenesRegression:\n",
    "        num_genes: 100\n",
    "      HighlyVariableGenesRawCount:\n",
    "        n_top_genes: 100\n",
    "      HighlyVariableGenesLogarithmizedByTopGenes:\n",
    "        n_top_genes: 100\n",
    "  - type: feature.cell\n",
    "    include:\n",
    "      - WeightedFeaturePCA\n",
    "      - WeightedFeatureSVD\n",
    "      - CellPCA\n",
    "      - CellSVD\n",
    "      - GaussRandProjFeature  # Registered custom preprocessing func\n",
    "      - FeatureCellPlaceHolder\n",
    "    params:\n",
    "      out: feature.cell\n",
    "      log_level: INFO\n",
    "  - type: misc\n",
    "    target: SetConfig\n",
    "    params:\n",
    "      config_dict:\n",
    "        feature_channel: feature.cell\n",
    "        label_channel: cell_type\n",
    "wandb:\n",
    "  entity: xzy11632\n",
    "  project: dance-dev\n",
    "  method: grid #try grid to provide a comprehensive search\n",
    "  metric:\n",
    "    name: acc  # val/acc\n",
    "    goal: maximize\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips**:\n",
    "\n",
    "1. In `tune_mode=pipeline`, the system will only tune the preprocessing pipeline.  \n",
    "2. In `tune_mode=params`, the system will only tune the model parameters.  \n",
    "3. In `tune_mode=pipeline_params`, the system will do a two-stage search: first for pipelines, then for model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Testing & Execution\n",
    "\n",
    "After setting everything up:\n",
    "\n",
    "```bash\n",
    "# Search only the best preprocessing pipeline:\n",
    "python main.py --tune_mode pipeline\n",
    "\n",
    "# Search only the best model hyperparameters:\n",
    "python main.py --tune_mode params\n",
    "\n",
    "# Joint two-stage search for both pipeline and parameters:\n",
    "python main.py --tune_mode pipeline_params\n",
    "```\n",
    "\n",
    "Once this completes, you should see results logged into Weights & Biases (wandb). The save_summary_data function writes out a CSV of the top performing runs. If you selected pipeline_params, the script also generates a default param config for the second stage of the search, which is automatically run via run_step3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "By following these steps:\n",
    "\n",
    "Inherit from the appropriate base class (in our case BaseClassificationMethod).\n",
    "Implement the fit, predict, and (optionally) preprocessing_pipeline methods.\n",
    "Integrate your custom model into the main.py script.\n",
    "Create and reference the necessary configuration (YAML) files.\n",
    "Run the pipeline using --tune_mode (pipeline|params|pipeline_params).\n",
    "…you can easily plug in any custom algorithm—ranging from simple classification methods like an SVM to deep learning methods with pretraining steps—into this auto-search framework.\n",
    "\n",
    "Happy coding and good luck with your hyperparameter searches!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
